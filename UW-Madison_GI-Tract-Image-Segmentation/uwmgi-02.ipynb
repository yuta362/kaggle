{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ft\nimport plotly.offline as offline\n\nimport os\nimport pathlib\nimport gc\nimport re\nimport math\nimport random\nimport time\nimport datetime as dt\nfrom tqdm import tqdm\nfrom pprint import pprint\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nimport torchvision.transforms.functional as TF\nfrom torchvision.models import resnet18\n!pip install torchinfo -q --user\nfrom torchinfo import summary\n\nfrom PIL import Image\n\nprint('import done!')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-26T23:53:37.88546Z","iopub.execute_input":"2022-06-26T23:53:37.885876Z","iopub.status.idle":"2022-06-26T23:53:54.367606Z","shell.execute_reply.started":"2022-06-26T23:53:37.885784Z","shell.execute_reply":"2022-06-26T23:53:54.366445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For reproducible results\ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed(s)\n    os.environ['PYTHONHASHSEED'] = str(s)\n    print('Sees setted!')\n\nglobal_seed = 42\nseed_all(global_seed)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:50.695824Z","iopub.execute_input":"2022-06-26T23:54:50.697975Z","iopub.status.idle":"2022-06-26T23:54:50.708404Z","shell.execute_reply.started":"2022-06-26T23:54:50.697919Z","shell.execute_reply":"2022-06-26T23:54:50.706611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Data Loading\ndata_config = {'train_csv_path': '../input/uw-madison-gi-tract-image-segmentation/train.csv',\n               'train_folder_path': '../input/uw-madison-gi-tract-image-segmentation/train',\n               'test_folder_path': '../input/uw-madison-gi-tract-image-segmentation/test',\n               'sample_submission_path': '../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv',\n              }\n\ntrain_df = pd.read_csv(data_config['train_csv_path'])\nsubmission_df = pd.read_csv(data_config['sample_submission_path'])\n\nprint(f'train_length: {len(train_df)}')\nprint(f'submission_length: {len(submission_df)}')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:53.726361Z","iopub.execute_input":"2022-06-26T23:54:53.727314Z","iopub.status.idle":"2022-06-26T23:54:54.278179Z","shell.execute_reply.started":"2022-06-26T23:54:53.72727Z","shell.execute_reply":"2022-06-26T23:54:54.277143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Null Value Check\nprint('train_df.info()'); print(train_df.info(), '\\n')\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:56.305599Z","iopub.execute_input":"2022-06-26T23:54:56.306074Z","iopub.status.idle":"2022-06-26T23:54:56.3888Z","shell.execute_reply.started":"2022-06-26T23:54:56.306034Z","shell.execute_reply":"2022-06-26T23:54:56.387883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Separate 'id' columns' texts, and create new id columns.\n## This code takes about 2 minutets to execute.\n\ndef create_id_list(text, p_train = pathlib.Path(data_config['train_folder_path'])):\n    t = text.split('_')\n    \n    case_id = t[0][4:]\n    day_id = t[1][3:]\n    slice_id = t[3]\n    \n    case_folder = t[0]\n    day_folder = ('_').join([t[0], t[1]])\n    slice_file = ('_').join([t[2], t[3]])\n    \n    p_folder = p_train / case_folder / day_folder / 'scans'\n    file_name = [p.name for p in p_folder.iterdir() if p.name[6:10] == slice_id]\n    id_list = [case_id, day_id, slice_id, case_folder, day_folder, slice_file]\n    id_list.extend(file_name)    \n    return id_list\n\ndef create_new_ids(dataframe, new_ids = ['case_id', 'day_id', 'slice_id', 'case_folder', 'day_folder', 'slice_file', 'file_name']):\n    dataframe['id_list'] = dataframe['id'].map(create_id_list)   \n    for i, item in enumerate(new_ids):\n        dataframe[item] = dataframe['id_list'].map(lambda x: x[i])\n    dataframe = dataframe.drop(['id_list'], axis=1)\n    return dataframe\n\ntrain_df = create_new_ids(train_df)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:54:58.644733Z","iopub.execute_input":"2022-06-26T23:54:58.645418Z","iopub.status.idle":"2022-06-26T23:56:33.735821Z","shell.execute_reply.started":"2022-06-26T23:54:58.645381Z","shell.execute_reply":"2022-06-26T23:56:33.734809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create detection column (1: non NaN segmentation, 0: NaN segmentation).\ntrain_df['detection'] = train_df['segmentation'].notna() * 1\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:01:58.500308Z","iopub.execute_input":"2022-06-27T00:01:58.500679Z","iopub.status.idle":"2022-06-27T00:01:58.538561Z","shell.execute_reply.started":"2022-06-27T00:01:58.500649Z","shell.execute_reply":"2022-06-27T00:01:58.537711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_img_n = int(len(train_df) / 3)\nprint('The number of imgs: ', total_img_n)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:02:08.788884Z","iopub.execute_input":"2022-06-27T00:02:08.789496Z","iopub.status.idle":"2022-06-27T00:02:08.795215Z","shell.execute_reply.started":"2022-06-27T00:02:08.789456Z","shell.execute_reply":"2022-06-27T00:02:08.793939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Calculate segmentation areas and img size.\ndef cal_pos_area(segmentation):\n    pos_area = 0\n    if type(segmentation) is str:\n        seg_list = segmentation.split(' ')\n        for i in range(len(seg_list)//2):\n            pos_area += int(seg_list[i*2 + 1])\n    return pos_area\n\ndef cal_total_area(file_name):\n    img_h = int(file_name[11:14])\n    img_w = int(file_name[15:18])\n    total_area = img_h * img_w\n    return total_area\n\ntrain_df['pos_area'] = train_df['segmentation'].map(cal_pos_area)\ntrain_df['total_area'] = train_df['file_name'].map(cal_total_area)\ntrain_df['pos_area_percentage'] = train_df['pos_area'] / train_df['total_area'] * 100\n\n## Check\ntrain_df[1920:1930]","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:02:18.688118Z","iopub.execute_input":"2022-06-27T00:02:18.688619Z","iopub.status.idle":"2022-06-27T00:02:20.639398Z","shell.execute_reply.started":"2022-06-27T00:02:18.688586Z","shell.execute_reply":"2022-06-27T00:02:20.638483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Split the samples based on the 'class'.\ntrain_lb_df = train_df[train_df['class']=='large_bowel'].reset_index(drop=True)\ntrain_sb_df = train_df[train_df['class']=='small_bowel'].reset_index(drop=True)\ntrain_st_df = train_df[train_df['class']=='stomach'].reset_index(drop=True)\n\n## Calculate each segmentation pixels' ratio to the total img pixels.\nlb_area_ratio = train_lb_df['pos_area'].sum() / train_lb_df['total_area'].sum()\nsb_area_ratio = train_sb_df['pos_area'].sum() / train_sb_df['total_area'].sum()\nst_area_ratio = train_st_df['pos_area'].sum() / train_st_df['total_area'].sum()\nbg_area_ratio = 1 - (lb_area_ratio + sb_area_ratio + st_area_ratio)\n\nprint(lb_area_ratio, sb_area_ratio, st_area_ratio, bg_area_ratio)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:02:35.309752Z","iopub.execute_input":"2022-06-27T00:02:35.310492Z","iopub.status.idle":"2022-06-27T00:02:35.453058Z","shell.execute_reply.started":"2022-06-27T00:02:35.310454Z","shell.execute_reply":"2022-06-27T00:02:35.45184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Split the samples which have non-null values in 'segmentation' as positive ones.\ntrain_positive_df = train_df.dropna(subset=['segmentation']).reset_index(drop=True)\ntrain_negative_df = train_df[train_df['segmentation'].isna()].reset_index(drop=True)\n\npos_lb_df = train_positive_df[train_positive_df['class']=='large_bowel'].reset_index(drop=True)\npos_sb_df = train_positive_df[train_positive_df['class']=='small_bowel'].reset_index(drop=True)\npos_st_df = train_positive_df[train_positive_df['class']=='stomach'].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:02:48.89816Z","iopub.execute_input":"2022-06-27T00:02:48.89879Z","iopub.status.idle":"2022-06-27T00:02:49.021001Z","shell.execute_reply.started":"2022-06-27T00:02:48.898753Z","shell.execute_reply":"2022-06-27T00:02:49.020037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Train - Valid - Test split\n## I split the train, valid, test data based on the case_id (imgs that have the same case_id are assigned in the same set).\n\ntrain_ratio = 0.85\nvalid_ratio = 0.10\ntest_ratio = 0.05\n\ncase_ids = train_df['case_id'].unique()\nidxs = np.random.permutation(range(len(case_ids)))\ncut_1 = int(train_ratio * len(idxs))\ncut_2 = int((train_ratio + valid_ratio) * len(idxs))\n\ntrain_case_ids = case_ids[idxs[:cut_1]]\nvalid_case_ids = case_ids[idxs[cut_1:cut_2]]\ntest_case_ids = case_ids[idxs[cut_2:]]\n\ntrain = train_df.query('case_id in @train_case_ids')\nvalid = train_df.query('case_id in @valid_case_ids')\ntest = train_df.query('case_id in @test_case_ids')\n\nprint(len(train), len(valid), len(test), len(train_df))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:56:58.587572Z","iopub.execute_input":"2022-06-26T23:56:58.587971Z","iopub.status.idle":"2022-06-26T23:56:58.679268Z","shell.execute_reply.started":"2022-06-26T23:56:58.587936Z","shell.execute_reply":"2022-06-26T23:56:58.678228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_case_folders = train['case_folder'].unique()\ntrain_files = []\nfor case_folder in train_case_folders:\n    p_train = pathlib.Path(data_config['train_folder_path'])\n    p_folder = p_train / case_folder\n    tmp_files = list(p_folder.glob('**/scans/*.png'))\n    train_files.extend(tmp_files)\n    \nvalid_case_folders = valid['case_folder'].unique()\nvalid_files = []\nfor case_folder in valid_case_folders:\n    p_train = pathlib.Path(data_config['train_folder_path'])\n    p_folder = p_train / case_folder\n    tmp_files = list(p_folder.glob('**/scans/*.png'))\n    valid_files.extend(tmp_files)\n    \ntest_case_folders = test['case_folder'].unique()\ntest_files = []\nfor case_folder in test_case_folders:\n    p_train = pathlib.Path(data_config['train_folder_path'])\n    p_folder = p_train / case_folder\n    tmp_files = list(p_folder.glob('**/scans/*.png'))\n    test_files.extend(tmp_files)\n    \nprint(len(train_files), len(valid_files), len(test_files))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:57:00.835575Z","iopub.execute_input":"2022-06-26T23:57:00.836323Z","iopub.status.idle":"2022-06-26T23:57:02.417246Z","shell.execute_reply.started":"2022-06-26T23:57:00.836288Z","shell.execute_reply":"2022-06-26T23:57:02.416316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Building Dataset and DataLoader\nclass UWMadison2022Dataset(torch.utils.data.Dataset):\n    def __init__(self, files, dataframe=None, input_shape=256,):\n        self.files = files\n        self.df = dataframe\n        self.input_shape = input_shape\n        self.transforms = transforms.Compose([\n            transforms.CenterCrop(self.input_shape),\n            transforms.Normalize(mean=[(0.485+0.456+0.406)/3], std=[(0.229+0.224+0.225)/3]),\n        ])\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        p_file = self.files[idx]\n        #img = torchvision.io.read_image(p_file)\n        img = np.array(Image.open(p_file))\n        img_shape = torch.tensor(img.shape)\n        img = transforms.functional.to_tensor(img) / 255.\n        img = self.transforms(img)\n        #img = torch.cat([img, img, img], dim=0)\n        \n        if self.df is not None:\n            f_name = str(p_file).split('/')\n            case_day_id = f_name[5]\n            slice_id = f_name[7][:10]\n            f_id = '_'.join([case_day_id, slice_id])\n            labels_df = self.df.query('id == @f_id')\n            \n            label = torch.zeros([img_shape[0]*img_shape[1]])\n            for i, organ in enumerate(['large_bowel', 'small_bowel', 'stomach']):\n                segmentation = labels_df[labels_df['class'] == organ]['segmentation'].item()\n                if type(segmentation) is str:\n                    segmentation = segmentation.split(' ')\n                    for j in range(len(segmentation)//2):\n                        start_idx = int(segmentation[j*2])\n                        span = int(segmentation[j*2 + 1])\n                        label[start_idx:(start_idx+span)] = (i+1)\n            label = torch.reshape(label, (img_shape[0], img_shape[1]))\n            label = transforms.CenterCrop(self.input_shape)(label)\n            label = torch.nn.functional.one_hot(label.to(torch.int64), num_classes=4)\n            label = label.permute(2, 0, 1)\n            return img, label, img_shape\n        \n        else: return img, img_shape\n\ntrain_ds = UWMadison2022Dataset(train_files, train, input_shape=256)\nvalid_ds = UWMadison2022Dataset(valid_files, valid, input_shape=256)\ntest_ds = UWMadison2022Dataset(test_files, test, input_shape=256)\n\nBATCH_SIZE = 32\n\n## Checking dataset and dataloder  \nprint('------ train_dl ------')\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\ntmp = train_dl.__iter__()\nx, y, shape = tmp.next()\nprint(f\"x : {x.shape}\")\nprint(f\"labels: {y.shape}\")\nprint(f\"img_shapes: {shape.shape}\")\nprint(f\"n_samples: {len(train_ds)}\")\nprint(f\"n_batches: {len(tmp)}\")\nprint()\n\nprint('------ valid_dl ------')\nvalid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\ntmp = valid_dl.__iter__()\nx, y, shape = tmp.next()\nprint(f\"x : {x.shape}\")\nprint(f\"labels: {y.shape}\")\nprint(f\"img_shapes: {shape.shape}\")\nprint(f\"n_samples: {len(valid_ds)}\")\nprint(f\"n_batches: {len(tmp)}\")\nprint()\n\nprint('------ test_dl ------')\ntest_dl = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\ntmp = test_dl.__iter__()\nx, y, shape = tmp.next()\nprint(f\"x : {x.shape}\")\nprint(f\"labels: {y.shape}\")\nprint(f\"img_shapes: {shape.shape}\")\nprint(f\"n_samples: {len(test_ds)}\")\nprint(f\"n_batches: {len(tmp)}\")\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:57:07.256299Z","iopub.execute_input":"2022-06-26T23:57:07.256727Z","iopub.status.idle":"2022-06-26T23:57:08.672791Z","shell.execute_reply.started":"2022-06-26T23:57:07.256694Z","shell.execute_reply":"2022-06-26T23:57:08.671788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unet","metadata":{}},{"cell_type":"code","source":"batch_size = 16","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:57:11.666135Z","iopub.execute_input":"2022-06-26T23:57:11.666476Z","iopub.status.idle":"2022-06-26T23:57:11.671607Z","shell.execute_reply.started":"2022-06-26T23:57:11.666449Z","shell.execute_reply":"2022-06-26T23:57:11.670365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## The Extractor of intermediate features of ResNet (encoder) for the skip connections to the decoder.\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.resnet = resnet18(pretrained=True)\n        # Change first conv layer to accept single-channel (grayscale) input\n        self.resnet.conv1.weight = torch.nn.Parameter(self.resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n        \n    def forward(self, x):\n        skip_connections = []\n        for i in range(8):\n            x = list(self.resnet.children())[i](x)\n            if i in [2, 4, 5, 6, 7]:\n                skip_connections.append(x)\n        encoder_outputs = skip_connections.pop(-1)\n        skip_connections = skip_connections[::-1]\n        \n        return encoder_outputs, skip_connections","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:57:13.827114Z","iopub.execute_input":"2022-06-26T23:57:13.827681Z","iopub.status.idle":"2022-06-26T23:57:13.840127Z","shell.execute_reply.started":"2022-06-26T23:57:13.827639Z","shell.execute_reply":"2022-06-26T23:57:13.838846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## The modules used for building the decoder architecture.\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.dconv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n        \n    def forward(self, x):\n        return  self.dconv(x)\n    \n    \nclass UnetUpSample(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.convt = nn.ConvTranspose2d(\n            in_channels, out_channels, kernel_size=2, stride=2)\n        self.norm1 = nn.BatchNorm2d(out_channels)\n        self.act1 = nn.ReLU(inplace=True)\n        self.dconv = DoubleConv(out_channels*2, out_channels)\n         \n    def forward(self, layer_input, skip_input):\n        u = self.convt(layer_input)\n        u = self.norm1(u)\n        u = self.act1(u)\n        u = torch.cat((u, skip_input), dim=1)\n        u = self.dconv(u)\n        return u","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:57:15.836007Z","iopub.execute_input":"2022-06-26T23:57:15.836367Z","iopub.status.idle":"2022-06-26T23:57:15.846664Z","shell.execute_reply.started":"2022-06-26T23:57:15.836339Z","shell.execute_reply":"2022-06-26T23:57:15.845747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## U-Net architechture.\nclass UW2022Unet(nn.Module):\n    def __init__(self, out_channels):\n        super().__init__()\n        self.encoder = FeatureExtractor()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.upsample1 = UnetUpSample(512, 256)\n        self.upsample2 = UnetUpSample(256, 128)\n        self.upsample3 = UnetUpSample(128, 64)\n        self.upsample4 = UnetUpSample(64, 64)\n        \n        self.final_convt = nn.ConvTranspose2d(\n            64, 64, kernel_size=2, stride=2)\n        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        x1, skip_connections = self.encoder(x)\n        x2 = self.upsample1(x1, skip_connections[0])\n        x3 = self.upsample2(x2, skip_connections[1])\n        x4 = self.upsample3(x3, skip_connections[2])\n        x5 = self.upsample4(x4, skip_connections[3])\n        x6 = self.final_convt(x5)\n        \n        return self.final_conv(x6)\n    \nmodel = UW2022Unet(out_channels=4)\n\nsummary(\n    model,\n    input_size=(batch_size, 1, 256, 256),\n    col_names=[\"output_size\", \"num_params\"],\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:24:01.597938Z","iopub.execute_input":"2022-06-27T00:24:01.598537Z","iopub.status.idle":"2022-06-27T00:24:01.927876Z","shell.execute_reply.started":"2022-06-27T00:24:01.598503Z","shell.execute_reply":"2022-06-27T00:24:01.926904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Swin-unet","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\n## Limit GPU Memory in TensorFlow\n## Because TensorFlow, by default, allocates the full amount of available GPU memory when it is launched. \nphysical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n    for device in physical_devices:\n        tf.config.experimental.set_memory_growth(device, True)\n        print('{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))\nelse:\n    print(\"Not enough GPU hardware devices available\")","metadata":{"execution":{"iopub.status.busy":"2022-06-26T23:57:32.095139Z","iopub.execute_input":"2022-06-26T23:57:32.095516Z","iopub.status.idle":"2022-06-26T23:57:36.338761Z","shell.execute_reply.started":"2022-06-26T23:57:32.095484Z","shell.execute_reply":"2022-06-26T23:57:36.337807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras-unet-collection -q -U\nfrom keras_unet_collection import models, losses\n\ntf_model = models.swin_unet_2d((256, 256, 1), filter_num_begin=64,\n                               n_labels=4, depth=4, stack_num_down=2, stack_num_up=2,\n                               patch_size=(4, 4), num_heads=[4, 8, 8, 8],\n                               window_size=[4, 2, 2, 2], num_mlp=512, \n                               output_activation='Softmax', shift_window=True,\n                               name='swin_unet')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:10:21.632637Z","iopub.execute_input":"2022-06-27T00:10:21.633029Z","iopub.status.idle":"2022-06-27T00:10:33.536032Z","shell.execute_reply.started":"2022-06-27T00:10:21.632999Z","shell.execute_reply":"2022-06-27T00:10:33.535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_model.compile(loss='binary_crossentropy',\n              optimizer=keras.optimizers.Adam(lr=1e-3),\n              metrics=['accuracy', losses.dice_coef])\ntf_model.summary()\n## To train this tf_model, we have to create TensorFlow Datasets.","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:10:35.342585Z","iopub.execute_input":"2022-06-27T00:10:35.342965Z","iopub.status.idle":"2022-06-27T00:10:35.37718Z","shell.execute_reply.started":"2022-06-27T00:10:35.342935Z","shell.execute_reply":"2022-06-27T00:10:35.376138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del tf_model\nkeras.backend.clear_session()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:08:56.370937Z","iopub.execute_input":"2022-06-27T00:08:56.371524Z","iopub.status.idle":"2022-06-27T00:08:56.379956Z","shell.execute_reply.started":"2022-06-27T00:08:56.37149Z","shell.execute_reply":"2022-06-27T00:08:56.378843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"## Focal Loss Function\nclass SegmentationFocalLoss(nn.Module):\n    def __init__(self, gamma=2, weight=None):\n        super().__init__()\n        self.gamma = gamma\n        if torch.cuda.is_available():\n            self.loss = torch.nn.CrossEntropyLoss(weight=weight).cuda()\n        else:\n            self.loss = nn.CrossEntropyLoss(weight=weight)\n\n    def forward(self, pred, target):\n        ce_loss = self.loss(pred, target)\n        #ce_loss = torch.nn.functional.cross_entropy(pred, target, reduce=False)\n        pt = torch.exp(-ce_loss)\n        focal_loss = (1. - pt) ** self.gamma * ce_loss\n        return torch.mean(focal_loss)\n\n##Setting the weight parameter of CrossEntropyLoss.\nlb_weight = 1 / lb_area_ratio\nsb_weight = 1 / sb_area_ratio\nst_weight = 1 / st_area_ratio\nbg_weight = 1 / bg_area_ratio\ntotal_weight = lb_weight + sb_weight + st_weight + bg_weight\n\nlb_weight = lb_weight / total_weight * 5\nsb_weight = sb_weight / total_weight * 5 \nst_weight = st_weight / total_weight * 5\nbg_weight = bg_weight / total_weight * 5\nweight = torch.tensor([bg_weight, lb_weight, sb_weight, st_weight], dtype=torch.float)\nprint(f'bg:{bg_weight}, lb:{lb_weight}, sb:{sb_weight}, st{st_weight}')\n\nloss_fn = SegmentationFocalLoss(gamma=3, weight=weight)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:10:43.981948Z","iopub.execute_input":"2022-06-27T00:10:43.982323Z","iopub.status.idle":"2022-06-27T00:10:43.995412Z","shell.execute_reply.started":"2022-06-27T00:10:43.982294Z","shell.execute_reply":"2022-06-27T00:10:43.99292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:26:36.096559Z","iopub.execute_input":"2022-06-27T00:26:36.096935Z","iopub.status.idle":"2022-06-27T00:26:36.102898Z","shell.execute_reply.started":"2022-06-27T00:26:36.096897Z","shell.execute_reply":"2022-06-27T00:26:36.101743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For the model training loop.\nif torch.cuda.is_available():\n    DEVICE = 'cuda'\nelse: DEVICE = 'cpu'\n\ndef train_fn(loader, model, optimizer, loss_fn, device=DEVICE):\n    model.train()\n    train_loss = 0.\n    loop = tqdm(loader)\n    \n    for batch_idx, (data, targets, img_size) in enumerate(loop):\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n        \n        predictions = model(data)\n        targets = torch.argmax(targets, dim=1)\n        loss = loss_fn(predictions, targets)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        loop.set_postfix(loss=loss.item())\n        train_loss += loss.detach().cpu().numpy() * BATCH_SIZE\n        \n    train_loss = train_loss / (BATCH_SIZE * len(train_dl))\n    return train_loss\n\n## For the model validation loop.\ndef valid_fn(loader, model, loss_fn, device=DEVICE):\n    model.eval()\n    valid_loss = 0.\n    loop = tqdm(loader)\n    \n    with torch.no_grad():\n        for batch_idx, (data, targets, img_size) in enumerate(loop):\n            data = data.to(device=device)\n            targets = targets.to(device=device)\n            \n            predictions = model(data)\n            targets = torch.argmax(targets, dim=1)\n            loss = loss_fn(predictions, targets)\n            valid_loss += loss * BATCH_SIZE\n            \n            loop.set_postfix(loss=loss.item())\n            \n        valid_loss = valid_loss / (BATCH_SIZE * len(valid_dl))\n    return valid_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:26:39.187375Z","iopub.execute_input":"2022-06-27T00:26:39.187755Z","iopub.status.idle":"2022-06-27T00:26:39.201432Z","shell.execute_reply.started":"2022-06-27T00:26:39.187721Z","shell.execute_reply":"2022-06-27T00:26:39.199938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For the train & validation loop.\nNUM_EPOCHS = 10\n\n## DeepLabv3 model\nmodel.to(device=DEVICE)\n\nbest_loss = 100\nfor epoch in range(NUM_EPOCHS):\n    print('-------------')\n    print('Epoch {}/{}'.format(epoch+1, NUM_EPOCHS))\n    print('-------------')\n    \n    train_loss = train_fn(train_dl, model, optimizer, loss_fn, DEVICE)\n    valid_loss = valid_fn(valid_dl, model, loss_fn, DEVICE)\n    \n    if valid_loss < best_loss:\n        checkpoint = {\n            'model': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }\n        torch.save(checkpoint, \"./checkpoint.pth\")\n        print('best model saved!')\n        best_loss = valid_loss\n    \n    print(f'Train Loss: {train_loss},  Valid Loss: {valid_loss}')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T00:26:55.267219Z","iopub.execute_input":"2022-06-27T00:26:55.268314Z","iopub.status.idle":"2022-06-27T02:27:56.445789Z","shell.execute_reply.started":"2022-06-27T00:26:55.268279Z","shell.execute_reply":"2022-06-27T02:27:56.444833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## prediction","metadata":{}},{"cell_type":"code","source":"checkpoint = torch.load(\"./checkpoint.pth\")\nmodel.load_state_dict(checkpoint[\"model\"])\noptimizer.load_state_dict(checkpoint[\"optimizer\"])\n        \nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dl):\n        x = batch[0].to(DEVICE)\n        test_pred = model(x)\n        test_pred = torch.argmax(test_pred, dim=1)\n        test_pred = torch.nn.functional.one_hot(test_pred, num_classes=4)\n        test_pred = torch.permute(test_pred, dims=[0, 3, 1, 2])\n        test_pred = test_pred[:, 1:, ...] ## We don't need background predictions.\n        test_pred = test_pred.detach().cpu().numpy()\n        predictions.append(test_pred)\n    \npredictions = np.concatenate(predictions, axis=0)\npredictions = predictions.reshape([-1, 256, 256])\nprint(predictions.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:49:16.170944Z","iopub.execute_input":"2022-06-27T02:49:16.171552Z","iopub.status.idle":"2022-06-27T02:49:47.029209Z","shell.execute_reply.started":"2022-06-27T02:49:16.171518Z","shell.execute_reply":"2022-06-27T02:49:47.028125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_encode(img):\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    encodes = ' '.join(str(x) for x in runs)\n    if encodes == '':\n        encodes = np.nan\n    return encodes\n\npredictions_rle = []\n\nfor pred in predictions:\n    pred_rle = rle_encode(pred)\n    predictions_rle.append(pred_rle)\n    \npredictions_rle = np.concatenate([predictions_rle], axis=0)\ntest['prediction'] = predictions_rle\n\ntest.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T02:49:53.4613Z","iopub.execute_input":"2022-06-27T02:49:53.461681Z","iopub.status.idle":"2022-06-27T02:49:54.944137Z","shell.execute_reply.started":"2022-06-27T02:49:53.461651Z","shell.execute_reply":"2022-06-27T02:49:54.943204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}